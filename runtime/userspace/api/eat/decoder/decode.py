#!/usr/bin/env python3
# Licensed under the Apache-2.0 license
"""
Simple COSE Sign1 / EAT Token Decoder

This script decodes COSE Sign1 messages (used for EAT tokens) without requiring
external dependencies like pycose or cbor2. It handles:

1. CBOR tags that wrap the COSE message (like tag 55799 for CWT)
2. COSE Sign1 structure parsing (4-element array)
3. EAT claims analysis with standard claim name mapping
4. Signature extraction and display

Usage:
    python3 decode.py

The script will attempt to decode both:
- example_eat_token.cbor
- structured_eat_token.cbor

Features:
- No external dependencies (pure Python)
- Handles CBOR tags automatically
- Shows standard EAT claim names (iss, cti, nonce, etc.)
- Displays hex dumps for binary data
- Works with OCP EAT tokens generated by the Rust encoder
"""

import os
import struct
import sys
import logging
import json
from signature_analysis import analyze_cose_signature, analyze_certificate_headers
from signature_validation import validate_cose_signature
from decode_json import extract_claims_to_json_only
from decode_eat_claims_json import parse_eat_claims_to_dict as structured_claims_parser

def parse_cbor_header(data, offset=0):
    """Parse CBOR header to understand the structure"""
    if offset >= len(data):
        return None, offset
    
    initial_byte = data[offset]
    major_type = (initial_byte >> 5) & 0x7
    additional_info = initial_byte & 0x1f
    
    offset += 1
    
    # Handle different additional info values
    if additional_info < 24:
        value = additional_info
    elif additional_info == 24:
        value = data[offset]
        offset += 1
    elif additional_info == 25:
        value = struct.unpack('>H', data[offset:offset+2])[0]
        offset += 2
    elif additional_info == 26:
        value = struct.unpack('>I', data[offset:offset+4])[0]
        offset += 4
    elif additional_info == 27:
        value = struct.unpack('>Q', data[offset:offset+8])[0]
        offset += 8
    else:
        value = additional_info
    
    return (major_type, value), offset

def cbor_type_to_string(major_type):
    """Convert CBOR major type number to string name"""
    type_names = {
        0: 'positive_int',
        1: 'negative_int', 
        2: 'byte_string',
        3: 'text_string',
        4: 'array',
        5: 'map',
        6: 'tag',
        7: 'simple'
    }
    return type_names.get(major_type, f'unknown_type_{major_type}')

def parse_cbor_header_with_names(data, offset=0):
    """Parse CBOR header and return named types"""
    header, new_offset = parse_cbor_header(data, offset)
    if header is None:
        return None, offset
    
    major_type, value = header
    type_name = cbor_type_to_string(major_type)
    return (type_name, value), new_offset

def skip_cbor_tags(data):
    """Skip CBOR tags to get to the actual COSE message"""
    offset = 0
    
    while offset < len(data):
        header, new_offset = parse_cbor_header(data, offset)
        if header is None:
            break
            
        major_type, value = header
        
        # Major type 6 is for tags
        if major_type == 6:
            print(f"Found CBOR tag: {value}")
            offset = new_offset
            continue
        else:
            # This is not a tag, so we've found our data
            return data[offset:]
    
    return data

def skip_cbor_array(data, offset, num_elements):
    """Skip over a CBOR array with the given number of elements"""
    for _ in range(num_elements):
        header, offset = parse_cbor_header(data, offset)
        if header:
            offset = skip_cbor_value_simple(data, offset, header[0], header[1])
        else:
            break
    return offset

def skip_cbor_map(data, offset, num_pairs):
    """Skip over a CBOR map with the given number of key-value pairs"""
    for _ in range(num_pairs * 2):  # Each pair has key and value
        header, offset = parse_cbor_header(data, offset)
        if header:
            offset = skip_cbor_value_simple(data, offset, header[0], header[1])
        else:
            break
    return offset

def skip_cbor_value_simple(data, offset, major_type, value):
    """Skip over a CBOR value of any type (simplified version)"""
    if major_type == 0 or major_type == 1 or major_type == 7:  # Integers and primitives
        return offset  # Already consumed
    elif major_type == 2 or major_type == 3:  # Byte string or text string
        return offset + value
    elif major_type == 4:  # Array
        return skip_cbor_array(data, offset, value)
    elif major_type == 5:  # Map
        return skip_cbor_map(data, offset, value)
    elif major_type == 6:  # Tag
        # Skip the tagged value
        header, offset = parse_cbor_header(data, offset)
        if header:
            offset = skip_cbor_value_simple(data, offset, header[0], header[1])
        return offset
    else:
        return offset

def parse_measurement_format(data, offset):
    """Parse MeasurementFormat structure (array format)"""
    try:
        array_info, new_offset = parse_cbor_header_with_names(data, offset)
        if array_info[0] != 'array':
            return f"Expected array, got {array_info[0]}", offset
        
        count = array_info[1]
        result = f"MeasurementFormat array ({count} elements):"
        current_offset = new_offset
        
        for i in range(count):
            if i == 0:
                # First element: content_type (integer)
                item_info, current_offset = parse_cbor_header_with_names(data, current_offset)
                if item_info[0] == 'positive_int':
                    result += f"\n          content_type: {item_info[1]} (CoAP Content-Format)"
                else:
                    result += f"\n          content_type: {item_info}"
            elif i == 1:
                # Second element: concise_evidence (byte string)
                item_info, current_offset = parse_cbor_header_with_names(data, current_offset)
                if item_info[0] == 'byte_string':
                    evidence_data = data[current_offset:current_offset + item_info[1]]
                    current_offset += item_info[1]
                    result += f"\n          concise_evidence: byte_string ({item_info[1]} bytes)"
                    
                    # Show hex dump of first 32 bytes for debugging
                    result += f"\n            Hex dump (first 32 bytes): {evidence_data[:32].hex()}"
                    
                    # Parse the concise evidence (may be tagged with CBOR tag 571)
                    try:
                        evidence_result = parse_concise_evidence(evidence_data, 0)
                        result += f"\n            ConciseEvidence:"
                        result += f"\n              {evidence_result[0]}"
                    except Exception as e:
                        result += f"\n            Failed to parse ConciseEvidence: {e}"
                else:
                    result += f"\n          concise_evidence: {item_info}"
            else:
                # Additional elements
                item_info, current_offset = parse_cbor_header_with_names(data, current_offset)
                result += f"\n          element_{i}: {item_info}"
        
        return result, current_offset
        
    except Exception as e:
        return f"Error parsing MeasurementFormat: {e}", offset

def parse_concise_evidence(data, offset):
    """Parse ConciseEvidence which may be tagged with CBOR tag 571"""
    try:
        # Check if this is tagged concise evidence (CBOR tag 571)
        first_info, new_offset = parse_cbor_header_with_names(data, offset)
        if first_info[0] == 'tag' and first_info[1] == 571:
            # This is tagged concise evidence
            result = f"Tagged ConciseEvidence (CBOR tag 571):"
            map_result = parse_concise_evidence_map(data, new_offset)
            result += f"\n  {map_result[0]}"
            return result, map_result[1]
        else:
            # This is untagged - parse directly as map
            result = f"Untagged ConciseEvidence:"
            map_result = parse_concise_evidence_map(data, offset)
            result += f"\n  {map_result[0]}"
            return result, map_result[1]
    except Exception as e:
        return f"Error parsing ConciseEvidence: {e}", offset

def parse_concise_evidence_map(data, offset):
    """Parse ConciseEvidenceMap structure"""
    try:
        map_info, new_offset = parse_cbor_header_with_names(data, offset)
        if map_info[0] != 'map':
            return f"Expected map, got {map_info[0]}", offset
        
        count = map_info[1]
        result = f"ConciseEvidenceMap with {count} entries:"
        current_offset = new_offset
        
        for i in range(count):
            # Parse key
            key_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            if key_info[0] == 'positive_int':
                key = key_info[1]
                
                if key == 0:  # environment
                    result += f"\n        environment:"
                    env_result, current_offset = parse_environment_map(data, current_offset)
                    result += f"\n          {env_result}"
                elif key == 1:  # class  
                    result += f"\n        class:"
                    class_result, current_offset = parse_class_map(data, current_offset)
                    result += f"\n          {class_result}"
                elif key == 2:  # measurements
                    result += f"\n        measurements:"
                    meas_result, current_offset = parse_evidence_measurements(data, current_offset)
                    result += f"\n          {meas_result}"
                else:
                    # Parse generic value
                    value_info, current_offset = parse_cbor_header_with_names(data, current_offset)
                    result += f"\n        key_{key}: {value_info}"
            else:
                result += f"\n        key_{key_info}: parsing error"
        
        return result, current_offset
        
    except Exception as e:
        return f"Error parsing ConciseEvidenceMap: {e}", offset

def parse_evidence_measurements(data, offset):
    """Parse evidence measurements array"""
    try:
        value_info, current_offset = parse_cbor_header_with_names(data, offset)
        if value_info[0] != 'array':
            return f"Expected array, got {value_info[0]}", offset
        
        measurements_count = value_info[1]
        result = f"array with {measurements_count} entries"
        
        # Parse each measurement entry
        for j in range(measurements_count):
            meas_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            if meas_info[0] == 'array':
                # Parse measurement record array
                meas_array_count = meas_info[1]
                result += f"\n          measurement_{j}: array with {meas_array_count} elements"
                # Typically: [environment-map, [measurement-maps]]
                for k in range(meas_array_count):
                    elem_info, current_offset = parse_cbor_header_with_names(data, current_offset)
                    if elem_info[0] == 'map':
                        result += f"\n            [{k}]: map with {elem_info[1]} entries"
                        # Parse map entries
                        for m in range(elem_info[1]):
                            key_info, current_offset = parse_cbor_header_with_names(data, current_offset)
                            val_info, current_offset = parse_cbor_header_with_names(data, current_offset)
                            result += f"\n              {key_info} -> {val_info}"
                    elif elem_info[0] == 'array':
                        result += f"\n            [{k}]: array with {elem_info[1]} elements"
                        # Skip array contents for now to avoid infinite recursion
                        for _ in range(elem_info[1]):
                            skip_info, current_offset = parse_cbor_header_with_names(data, current_offset)
                            result += f"\n              array_item: {skip_info}"
                    else:
                        result += f"\n            [{k}]: {elem_info}"
            else:
                result += f"\n          measurement_{j}: {meas_info}"
        
        return result, current_offset
        
    except Exception as e:
        return f"Error parsing evidence measurements: {e}", offset

def parse_class_array_entry(data, offset, class_index):
    """Parse a single class array entry"""
    try:
        class_info, current_offset = parse_cbor_header_with_names(data, offset)
        if class_info[0] != 'array':
            return f"class_{class_index}: {class_info}", offset
        
        array_count = class_info[1] 
        result = f"class_{class_index}: array with {array_count} elements"
        
        for k in range(array_count):
            elem_result, current_offset = parse_class_array_element(data, current_offset, k)
            result += f"\n            {elem_result}"
        
        return result, current_offset
        
    except Exception as e:
        return f"Error parsing class array entry {class_index}: {e}", offset

def parse_class_array_element(data, offset, elem_index):
    """Parse elements within a class array"""
    try:
        elem_info, current_offset = parse_cbor_header_with_names(data, offset)
        
        if elem_info[0] == 'map':
            map_result, current_offset = parse_nested_map(data, current_offset, elem_info[1], elem_index)
            return f"[{elem_index}]: {map_result}", current_offset
        elif elem_info[0] == 'array':
            array_result, current_offset = parse_measurement_array(data, current_offset, elem_info[1])
            return f"[{elem_index}]: {array_result}", current_offset
        elif elem_info[0] == 'text_string':
            text_value = data[current_offset:current_offset + elem_info[1]].decode('utf-8', errors='ignore')
            current_offset += elem_info[1]
            return f"[{elem_index}]: '{text_value}'", current_offset
        elif elem_info[0] == 'byte_string':
            bytes_value = data[current_offset:current_offset + elem_info[1]]
            current_offset += elem_info[1]
            return f"[{elem_index}]: bytes({elem_info[1]}) = {bytes_value.hex()}", current_offset
        elif elem_info[0] == 'positive_int':
            return f"[{elem_index}]: {elem_info[1]}", current_offset
        else:
            return f"[{elem_index}]: {elem_info}", current_offset
            
    except Exception as e:
        return f"Error parsing class array element {elem_index}: {e}", offset

def parse_nested_map(data, offset, map_count, context_index):
    """Parse nested maps within class arrays"""
    try:
        result = f"map with {map_count} entries"
        current_offset = offset
        
        for m in range(map_count):
            # Parse key
            key_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            key_value = None
            if key_info[0] == 'text_string':
                key_value = data[current_offset:current_offset + key_info[1]].decode('utf-8', errors='ignore')
                current_offset += key_info[1]
            elif key_info[0] == 'positive_int':
                key_value = key_info[1]
            
            # Parse value
            val_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            
            if val_info[0] == 'map':
                # Parse ClassMap or similar nested structures
                inner_map_result, current_offset = parse_class_map_fields(data, current_offset, val_info[1])
                result += f"\n              {key_value}: {inner_map_result}"
            elif val_info[0] == 'text_string':
                val_value = data[current_offset:current_offset + val_info[1]].decode('utf-8', errors='ignore')
                current_offset += val_info[1]
                result += f"\n              {key_value}: '{val_value}'"
            elif val_info[0] == 'byte_string':
                val_value = data[current_offset:current_offset + val_info[1]]
                current_offset += val_info[1]
                result += f"\n              {key_value}: bytes = {val_value.hex()}"
            elif val_info[0] == 'positive_int':
                result += f"\n              {key_value}: {val_info[1]}"
            else:
                result += f"\n              {key_value}: {val_info}"
        
        return result, current_offset
        
    except Exception as e:
        return f"Error parsing nested map: {e}", offset

def parse_class_map_fields(data, offset, inner_map_count):
    """Parse ClassMap fields with meaningful names"""
    try:
        result = f"map with {inner_map_count} entries"
        current_offset = offset
        
        for n in range(inner_map_count):
            inner_key_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            inner_key_value = None
            if inner_key_info[0] == 'text_string':
                inner_key_value = data[current_offset:current_offset + inner_key_info[1]].decode('utf-8', errors='replace')
                current_offset += inner_key_info[1]
            elif inner_key_info[0] == 'positive_int':
                inner_key_value = inner_key_info[1]
            else:
                inner_key_value = f"key_type_{inner_key_info[0]}"
            
            inner_val_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            
            # Map ClassMap keys to meaningful names
            field_name = inner_key_value
            if inner_key_value == 0:
                field_name = "class_id"
            elif inner_key_value == 1:
                field_name = "vendor" 
            elif inner_key_value == 2:
                field_name = "model"
            
            if inner_val_info[0] == 'text_string':
                inner_val_value = data[current_offset:current_offset + inner_val_info[1]].decode('utf-8', errors='replace')
                current_offset += inner_val_info[1]
                result += f"\n                {field_name}: '{inner_val_value}'"
            elif inner_val_info[0] == 'byte_string':
                inner_val_value = data[current_offset:current_offset + inner_val_info[1]]
                current_offset += inner_val_info[1]
                result += f"\n                {field_name}: bytes = {inner_val_value.hex()}"
            elif inner_val_info[0] == 'positive_int':
                result += f"\n                {field_name}: {inner_val_info[1]}"
            elif inner_val_info[0] == 'tag':
                # Handle CBOR tags (like tag 111 for class_id)
                tag_num = inner_val_info[1] 
                tagged_content, current_offset = parse_cbor_header_with_names(data, current_offset)
                if tagged_content[0] in ['text_string', 'byte_string']:
                    tagged_value = data[current_offset:current_offset + tagged_content[1]].decode('utf-8', errors='replace')
                    current_offset += tagged_content[1]
                    result += f"\n                {field_name}: '{tagged_value}' (tag {tag_num})"
                else:
                    result += f"\n                {field_name}: tag({tag_num}) -> {tagged_content}"
            else:
                result += f"\n                {field_name}: {inner_val_info}"
        
        return result, current_offset
        
    except Exception as e:
        return f"Error parsing class map fields: {e}", offset

def parse_measurement_array(data, offset, array_elem_count):
    """Parse measurement arrays within environment maps"""
    try:
        result = f"array with {array_elem_count} elements"
        current_offset = offset
        
        for j in range(array_elem_count):
            measurement_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            
            if measurement_info[0] == 'map':
                # Parse measurement map with detailed structure
                meas_result, current_offset = parse_measurement_map_detailed(data, current_offset, measurement_info[1], j)
                result += f"\n              {meas_result}"
            elif measurement_info[0] == 'array':
                # Parse nested measurement arrays
                nested_result, current_offset = parse_nested_measurement_array(data, current_offset, measurement_info[1], j)
                result += f"\n              {nested_result}"
            elif measurement_info[0] == 'text_string':
                text_val = data[current_offset:current_offset + measurement_info[1]].decode('utf-8', errors='replace')
                current_offset += measurement_info[1]
                result += f"\n              measurement_{j}: '{text_val}'"
            elif measurement_info[0] == 'byte_string':
                bytes_val = data[current_offset:current_offset + measurement_info[1]]
                current_offset += measurement_info[1]
                result += f"\n              measurement_{j}: bytes = {bytes_val.hex()}"
            elif measurement_info[0] == 'positive_int':
                result += f"\n              measurement_{j}: {measurement_info[1]}"
            elif measurement_info[0] == 'negative_int':
                result += f"\n              measurement_{j}: {-measurement_info[1] - 1}"
            else:
                result += f"\n              measurement_{j}: {measurement_info}"
        
        return result, current_offset
        
    except Exception as e:
        return f"Error parsing measurement array: {e}", offset

def parse_measurement_map_detailed(data, offset, meas_map_count, meas_index):
    """Parse detailed measurement map structure"""
    try:
        result = f"measurement_{meas_index}: map with {meas_map_count} entries"
        current_offset = offset
        
        for mm in range(meas_map_count):
            # Parse measurement key
            meas_key_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            if meas_key_info[0] == 'positive_int':
                meas_key_value = meas_key_info[1]
                # Map measurement keys to names
                if meas_key_value == 0:
                    meas_key_name = "mkey"
                elif meas_key_value == 1:
                    meas_key_name = "mval"
                else:
                    meas_key_name = f"key_{meas_key_value}"
            elif meas_key_info[0] == 'text_string':
                meas_key_name = data[current_offset:current_offset + meas_key_info[1]].decode('utf-8', errors='replace')
                current_offset += meas_key_info[1]
            else:
                meas_key_name = str(meas_key_info)
            
            # Parse measurement value
            meas_val_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            
            if meas_val_info[0] == 'map':
                # Parse the map contents (firmware measurement details)
                mval_result, current_offset = parse_measurement_value_map(data, current_offset, meas_val_info[1], meas_key_name)
                result += f"\n                {mval_result}"
            elif meas_val_info[0] == 'byte_string':
                meas_val_data = data[current_offset:current_offset + meas_val_info[1]]
                current_offset += meas_val_info[1]
                try:
                    meas_val_str = meas_val_data.decode('utf-8', errors='replace')
                    result += f"\n                {meas_key_name}: '{meas_val_str}'"
                except:
                    result += f"\n                {meas_key_name}: bytes = {meas_val_data.hex()}"
            elif meas_val_info[0] == 'text_string':
                meas_val_str = data[current_offset:current_offset + meas_val_info[1]].decode('utf-8', errors='replace')
                current_offset += meas_val_info[1]
                result += f"\n                {meas_key_name}: '{meas_val_str}'"
            elif meas_val_info[0] == 'positive_int':
                result += f"\n                {meas_key_name}: {meas_val_info[1]}"
            else:
                result += f"\n                {meas_key_name}: {meas_val_info}"
        
        return result, current_offset
        
    except Exception as e:
        return f"Error parsing measurement map {meas_index}: {e}", offset

def parse_measurement_value_map(data, offset, mval_map_count, field_name):
    """Parse measurement value map with field details"""
    try:
        result = f"{field_name}: map with {mval_map_count} entries"
        current_offset = offset
        
        for mv in range(mval_map_count):
            # Parse map key
            mv_key_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            if mv_key_info[0] == 'positive_int':
                mv_key_value = mv_key_info[1]
                # Common measurement value keys
                if mv_key_value == 0:
                    mv_key_name = "version"
                elif mv_key_value == 1:
                    mv_key_name = "svn"
                elif mv_key_value == 2:
                    mv_key_name = "digests"
                elif mv_key_value == 3:
                    mv_key_name = "flags"
                elif mv_key_value == 4:
                    mv_key_name = "raw_value"
                elif mv_key_value == 14:
                    mv_key_name = "integrity_registers"
                else:
                    mv_key_name = f"field_{mv_key_value}"
            elif mv_key_info[0] == 'text_string':
                mv_key_name = data[current_offset:current_offset + mv_key_info[1]].decode('utf-8', errors='replace')
                current_offset += mv_key_info[1]
            else:
                mv_key_name = str(mv_key_info)
            
            # Parse map value
            mv_val_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            
            if mv_val_info[0] == 'text_string':
                mv_val_str = data[current_offset:current_offset + mv_val_info[1]].decode('utf-8', errors='replace')
                current_offset += mv_val_info[1]
                result += f"\n                  {mv_key_name}: '{mv_val_str}'"
            elif mv_val_info[0] == 'byte_string':
                mv_val_bytes = data[current_offset:current_offset + mv_val_info[1]]
                current_offset += mv_val_info[1]
                result += f"\n                  {mv_key_name}: bytes = {mv_val_bytes.hex()}"
            elif mv_val_info[0] == 'positive_int':
                result += f"\n                  {mv_key_name}: {mv_val_info[1]}"
            elif mv_val_info[0] == 'array':
                # Parse arrays like signer_id or integrity registers
                array_result, current_offset = parse_measurement_value_array(data, current_offset, mv_val_info[1], mv_key_name)
                result += f"\n                  {array_result}"
            elif mv_val_info[0] == 'map' and mv_key_name == "integrity_registers":
                # Parse integrity registers map
                ir_result, current_offset = parse_integrity_registers_map(data, current_offset, mv_val_info[1])
                result += f"\n                  {ir_result}"
            else:
                result += f"\n                  {mv_key_name}: {mv_val_info}"
        
        return result, current_offset
        
    except Exception as e:
        return f"Error parsing measurement value map: {e}", offset

def parse_measurement_value_array(data, offset, mv_array_count, field_name):
    """Parse arrays within measurement values (like signer_id)"""
    try:
        result = f"{field_name}: array with {mv_array_count} elements"
        current_offset = offset
        
        for mvx in range(mv_array_count):
            mvx_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            
            if mvx_info[0] == 'text_string':
                mvx_val = data[current_offset:current_offset + mvx_info[1]].decode('utf-8', errors='replace')
                current_offset += mvx_info[1]
                result += f"\n                    [{mvx}]: '{mvx_val}'"
            elif mvx_info[0] == 'byte_string':
                mvx_bytes = data[current_offset:current_offset + mvx_info[1]]
                current_offset += mvx_info[1]
                result += f"\n                    [{mvx}]: bytes = {mvx_bytes.hex()}"
            elif mvx_info[0] == 'positive_int':
                result += f"\n                    [{mvx}]: {mvx_info[1]}"
            elif mvx_info[0] == 'negative_int':
                result += f"\n                    [{mvx}]: {-mvx_info[1] - 1}"
            elif mvx_info[0] == 'array':
                # Parse nested arrays within signer_id
                nested_result, current_offset = parse_nested_array_elements(data, current_offset, mvx_info[1], mvx)
                result += f"\n                    {nested_result}"
            else:
                result += f"\n                    [{mvx}]: {mvx_info}"
        
        return result, current_offset
        
    except Exception as e:
        return f"Error parsing measurement value array: {e}", offset

def parse_nested_array_elements(data, offset, mvx_sub_count, parent_index):
    """Parse nested array elements within measurement arrays"""
    try:
        result = f"[{parent_index}]: array with {mvx_sub_count} elements"
        current_offset = offset
        
        # Check if this looks like a digest entry (2 elements: algorithm + value)
        if mvx_sub_count == 2:
            # Parse algorithm ID (first element)
            alg_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            alg_name = "unknown"
            
            if alg_info[0] == 'positive_int':
                alg_id = alg_info[1]
                if alg_id == 1:
                    alg_name = "SHA-256"
                elif alg_id == 7:
                    alg_name = "SHA-384"
                elif alg_id == 8:
                    alg_name = "SHA-512"
                else:
                    alg_name = f"alg_{alg_id}"
            else:
                alg_name = f"unknown_type_{alg_info[0]}"
            
            result += f"\n                      [0]: {alg_name}"
            
            # Parse digest value (second element)
            val_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            if val_info[0] == 'byte_string':
                val_bytes = data[current_offset:current_offset + val_info[1]]
                current_offset += val_info[1]
                result += f"\n                      [1]: {alg_name} digest = {val_bytes.hex()}"
            else:
                result += f"\n                      [1]: {val_info}"
        else:
            # Parse as generic array elements
            for mvxs in range(mvx_sub_count):
                mvxs_info, current_offset = parse_cbor_header_with_names(data, current_offset)
                
                if mvxs_info[0] == 'negative_int':
                    result += f"\n                      [{mvxs}]: {-mvxs_info[1] - 1}"
                elif mvxs_info[0] == 'byte_string':
                    mvxs_bytes = data[current_offset:current_offset + mvxs_info[1]]
                    current_offset += mvxs_info[1]
                    result += f"\n                      [{mvxs}]: bytes = {mvxs_bytes.hex()}"
                elif mvxs_info[0] == 'text_string':
                    mvxs_val = data[current_offset:current_offset + mvxs_info[1]].decode('utf-8', errors='replace')
                    current_offset += mvxs_info[1]
                    result += f"\n                      [{mvxs}]: '{mvxs_val}'"
                else:
                    result += f"\n                      [{mvxs}]: {mvxs_info}"
        
        return result, current_offset
        
    except Exception as e:
        return f"Error parsing nested array elements: {e}", offset

def parse_integrity_registers_map(data, offset, ir_map_count):
    """Parse integrity registers map structure"""
    try:
        result = f"integrity_registers: map with {ir_map_count} entries"
        current_offset = offset
        
        for ir in range(ir_map_count):
            # Parse register ID (key)
            reg_id_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            reg_id_name = "unknown"
            if reg_id_info[0] == 'positive_int':
                reg_id_name = f"register_{reg_id_info[1]}"
            elif reg_id_info[0] == 'text_string':
                reg_id_name = data[current_offset:current_offset + reg_id_info[1]].decode('utf-8', errors='replace')
                current_offset += reg_id_info[1]
            
            # Parse digests array (value)
            reg_val_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            if reg_val_info[0] == 'array':
                digest_count = reg_val_info[1]
                result += f"\n                    {reg_id_name}: array with {digest_count} digests"
                for dig in range(digest_count):
                    # Parse digest entry [alg_id, value]
                    digest_info, current_offset = parse_cbor_header_with_names(data, current_offset)
                    if digest_info[0] == 'array' and digest_info[1] == 2:
                        digest_result, current_offset = parse_digest_entry(data, current_offset, dig)
                        result += f"\n                      {digest_result}"
                    else:
                        result += f"\n                      digest_{dig}: {digest_info}"
            else:
                result += f"\n                    {reg_id_name}: {reg_val_info}"
        
        return result, current_offset
        
    except Exception as e:
        return f"Error parsing integrity registers map: {e}", offset

def parse_digest_entry(data, offset, digest_index):
    """Parse individual digest entries with algorithm and value"""
    try:
        # Parse algorithm ID
        alg_info, current_offset = parse_cbor_header_with_names(data, offset)
        alg_name = "unknown"
        if alg_info[0] == 'positive_int':
            alg_id = alg_info[1]
            if alg_id == 1:
                alg_name = "SHA-256"
            elif alg_id == 7:
                alg_name = "SHA-384"
            elif alg_id == 8:
                alg_name = "SHA-512"
            else:
                alg_name = f"alg_{alg_id}"
        else:
            alg_name = f"unknown_type_{alg_info[0]}"
        
        
        # Parse digest value
        val_info, current_offset = parse_cbor_header_with_names(data, current_offset)
        if val_info[0] == 'byte_string':
            digest_bytes = data[current_offset:current_offset + val_info[1]]
            current_offset += val_info[1]
            result = f"digest_{digest_index}: {alg_name} = {digest_bytes.hex()}"
        else:
            result = f"digest_{digest_index}: {alg_name} = {val_info}"
        
        return result, current_offset
        
    except Exception as e:
        return f"Error parsing digest entry {digest_index}: {e}", offset

def parse_nested_measurement_array(data, offset, meas_sub_array_count, meas_index):
    """Parse nested measurement arrays"""
    try:
        result = f"measurement_{meas_index}: array with {meas_sub_array_count} elements"
        current_offset = offset
        
        for msub in range(meas_sub_array_count):
            msub_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            
            if msub_info[0] == 'text_string':
                msub_val = data[current_offset:current_offset + msub_info[1]].decode('utf-8', errors='replace')
                current_offset += msub_info[1]
                result += f"\n                [{msub}]: '{msub_val}'"
            elif msub_info[0] == 'byte_string':
                msub_bytes = data[current_offset:current_offset + msub_info[1]]
                current_offset += msub_info[1]
                result += f"\n                [{msub}]: bytes = {msub_bytes.hex()}"
            elif msub_info[0] == 'positive_int':
                result += f"\n                [{msub}]: {msub_info[1]}"
            elif msub_info[0] == 'negative_int':
                result += f"\n                [{msub}]: {-msub_info[1] - 1}"
            elif msub_info[0] == 'map':
                # Parse simple map structures
                msub_map_result, current_offset = parse_simple_map(data, current_offset, msub_info[1], msub)
                result += f"\n                {msub_map_result}"
            else:
                result += f"\n                [{msub}]: {msub_info}"
        
        return result, current_offset
        
    except Exception as e:
        return f"Error parsing nested measurement array: {e}", offset

def parse_simple_map(data, offset, msub_map_count, elem_index):
    """Parse simple map structures within arrays"""
    try:
        result = f"[{elem_index}]: map with {msub_map_count} entries"
        current_offset = offset
        
        for msmap in range(msub_map_count):
            msmap_key_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            msmap_val_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            
            msmap_key_name = "unknown"
            if msmap_key_info[0] == 'positive_int':
                if msmap_key_info[1] == 0:
                    msmap_key_name = "mkey"
                elif msmap_key_info[1] == 1:
                    msmap_key_name = "mval"
            elif msmap_key_info[0] == 'text_string':
                msmap_key_name = data[current_offset-msmap_key_info[1]:current_offset].decode('utf-8', errors='replace')
            
            if msmap_val_info[0] == 'text_string':
                msmap_val = data[current_offset:current_offset + msmap_val_info[1]].decode('utf-8', errors='replace')
                current_offset += msmap_val_info[1]
                result += f"\n                  {msmap_key_name}: '{msmap_val}'"
            else:
                result += f"\n                  {msmap_key_name}: {msmap_val_info}"
        
        return result, current_offset
        
    except Exception as e:
        return f"Error parsing simple map: {e}", offset

def parse_environment_map(data, offset):
    """Parse an EnvironmentMap structure (now modular)"""
    try:
        map_info, new_offset = parse_cbor_header_with_names(data, offset)
        if map_info[0] != 'map':
            return f"Expected map, got {map_info[0]}", offset
        
        count = map_info[1]
        result = f"EnvironmentMap with {count} entries:"
        current_offset = new_offset
        
        for i in range(count):
            # Parse key
            key_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            if key_info[0] == 'positive_int':
                key = key_info[1]
                
                if key == 0:  # class
                    # Parse value
                    value_info, current_offset = parse_cbor_header_with_names(data, current_offset)
                    if value_info[0] == 'array':
                        class_count = value_info[1]
                        result += f"\n          class: array with {class_count} entries"
                        # Parse each class entry using modular functions
                        for j in range(class_count):
                            class_result, current_offset = parse_class_array_entry(data, current_offset, j)
                            result += f"\n            {class_result}"
                    else:
                        result += f"\n          class: {value_info}"
                else:
                    # Parse generic value
                    value_info, current_offset = parse_cbor_header_with_names(data, current_offset)
                    result += f"\n          key_{key}: {value_info}"
            else:
                result += f"\n          key_{key_info}: parsing error"
                
        return result, current_offset
                        
    except Exception as e:
        return f"Error parsing EnvironmentMap: {e}", offset

def parse_class_map(data, offset):
    """Parse a ClassMap structure"""
    try:
        map_info, new_offset = parse_cbor_header_with_names(data, offset)
        if map_info[0] != 'map':
            return f"Expected map, got {map_info[0]}", offset
        
        count = map_info[1]
        result = f"ClassMap with {count} entries:"
        current_offset = new_offset
        
        for i in range(count):
            # Parse key
            key_info, current_offset = parse_cbor_header_with_names(data, current_offset)
            if key_info[0] == 'positive_int':
                key = key_info[1]
                
                # Parse value
                value_info, current_offset = parse_cbor_header_with_names(data, current_offset)
                
                if key == 0:  # class_id
                    result += f"\n          class_id: {value_info}"
                elif key == 1:  # vendor
                    result += f"\n          vendor: {value_info}"
                elif key == 2:  # model
                    result += f"\n          model: {value_info}"
                else:
                    result += f"\n          key_{key}: {value_info}"
            else:
                result += f"\n          key_{key_info}: parsing error"
                        
        return result, current_offset
                        
    except Exception as e:
        return f"Error parsing ClassMap: {e}", offset

def parse_measurements_array(data, offset, num_elements):
    """Parse measurements array"""
    try:
        print(f"              measurements: array ({num_elements} elements)")
        for elem_idx in range(num_elements):
            print(f"                Measurement {elem_idx + 1}:")
            header, offset = parse_cbor_header(data, offset)
            if header:
                offset = skip_cbor_value_simple(data, offset, header[0], header[1])
                
    except Exception as e:
        print(f"              Error parsing measurements: {e}")
        
    return offset

def get_concise_evidence_key_name(key):
    """Get names for ConciseEvidenceMap keys"""
    keys = {
        0: "environment",
        1: "measurements",
    }
    return keys.get(key, f"key_{key}")

def get_cbor_type_name(major_type):
    """Get readable name for CBOR major types"""
    types = {
        0: "positive_integer",
        1: "negative_integer", 
        2: "byte_string",
        3: "text_string",
        4: "array",
        5: "map",
        6: "tag",
        7: "primitive"
    }
    return types.get(major_type, f"type_{major_type}")

def get_eat_claim_name(key):
    """Get the standard name for EAT claim keys (matching eat_encoder.rs constants)"""
    eat_claims = {
        # Standard CWT/EAT claims
        1: "iss (Issuer)",
        2: "sub (Subject)", 
        3: "aud (Audience)",
        4: "exp (Expiration Time)",
        5: "nbf (Not Before)",
        6: "iat (Issued At)",
        7: "cti (CWT ID)",
        8: "cnf (Confirmation)",
        10: "nonce",
        
        # EAT-specific claims
        256: "ueid (Universal Entity ID)",
        257: "sueids (Semi-permanent UEIDs)",
        258: "oemid (OEM ID)",
        259: "hwmodel (Hardware Model)",
        260: "hwversion (Hardware Version)",
        261: "uptime (Uptime)",
        262: "swversion (Software Version)",
        263: "dbgstat (Debug Status)",
        264: "location",
        265: "eat_profile (EAT Profile)",
        266: "profile (Profile)",
        267: "bootcount (Boot Count)",
        268: "bootseed (Boot Seed)",
        269: "dloas (DLOA)",
        273: "measurements (Evidence)",
        
        # Private/custom claims
        -70001: "rim_locators (RIM Locators)",
        -70002: "private_claim_1",
        -70003: "private_claim_2", 
        -70004: "private_claim_3",
        -70005: "private_claim_4",
        -70006: "private_claim_5",
    }
    return eat_claims.get(key, f"claim-{key}")

def parse_cose_protected_headers(data, offset):
    """Parse COSE Sign1 protected headers"""
    header, new_offset = parse_cbor_header(data, offset)
    if header and header[0] == 2:  # Byte string
        protected_len = header[1]
        protected_headers = data[new_offset:new_offset+protected_len]
        print(f"Protected headers ({protected_len} bytes): {protected_headers.hex()}")
        return new_offset + protected_len
    return offset

def parse_cose_protected_headers_with_data(data, offset):
    """Parse COSE Sign1 protected headers and return data"""
    header, new_offset = parse_cbor_header(data, offset)
    if header and header[0] == 2:  # Byte string
        protected_len = header[1]
        protected_headers = data[new_offset:new_offset+protected_len]
        print(f"Protected headers ({protected_len} bytes): {protected_headers.hex()}")
        return new_offset + protected_len, protected_headers
    return offset, None

def parse_cose_unprotected_headers(data, offset):
    """Parse COSE Sign1 unprotected headers"""
    header, new_offset = parse_cbor_header(data, offset)
    if header and header[0] == 5:  # Map
        map_pairs = header[1]
        print(f"Unprotected headers: map with {map_pairs} pairs")
        # Parse the map content
        current_offset = new_offset
        for _ in range(map_pairs):  # key-value pairs
            # Parse key
            key_header, current_offset = parse_cbor_header(data, current_offset)
            key_value = None
            if key_header and key_header[0] == 0:  # Positive integer
                key_value = key_header[1]
            
            # Parse value
            value_header, current_offset = parse_cbor_header(data, current_offset)
            if value_header and value_header[0] == 2:  # Byte string
                value_data = data[current_offset:current_offset+value_header[1]]
                current_offset += value_header[1]
                
                print(f"  Key {key_value}: {len(value_data)} bytes")
                # Check if this is a certificate (X5CHAIN)
                if analyze_certificate_headers(key_value, value_data):
                    print(f"    Certificate analysis completed")
                else:
                    print(f"    Data: {value_data[:16].hex()}{'...' if len(value_data) > 16 else ''}")
            
        return current_offset
    return offset

def parse_cose_unprotected_headers_with_data(data, offset):
    """Parse COSE Sign1 unprotected headers and return certificate data"""
    header, new_offset = parse_cbor_header(data, offset)
    certificate_data = None
    
    if header and header[0] == 5:  # Map
        map_pairs = header[1]
        print(f"Unprotected headers: map with {map_pairs} pairs")
        # Parse the map content
        current_offset = new_offset
        for _ in range(map_pairs):  # key-value pairs
            # Parse key
            key_header, current_offset = parse_cbor_header(data, current_offset)
            key_value = None
            if key_header and key_header[0] == 0:  # Positive integer
                key_value = key_header[1]
            
            # Parse value
            value_header, current_offset = parse_cbor_header(data, current_offset)
            if value_header and value_header[0] == 2:  # Byte string
                value_data = data[current_offset:current_offset+value_header[1]]
                current_offset += value_header[1]
                
                print(f"  Key {key_value}: {len(value_data)} bytes")
                # Check if this is a certificate (X5CHAIN)
                if analyze_certificate_headers(key_value, value_data):
                    print(f"    Certificate analysis completed")
                    if key_value == 33:  # X5CHAIN
                        certificate_data = value_data
                else:
                    print(f"    Data: {value_data[:16].hex()}{'...' if len(value_data) > 16 else ''}")
            
        return current_offset, certificate_data
    return offset, None

def parse_cose_signature(data, offset):
    """Parse COSE Sign1 signature"""
    if offset < len(data):
        header, new_offset = parse_cbor_header(data, offset)
        if header and header[0] == 2:  # Byte string
            signature_len = header[1]
            signature = data[new_offset:new_offset+signature_len]
            print(f"Signature ({signature_len} bytes): {signature.hex()}")
            # Enhanced signature analysis
            analyze_cose_signature(signature)
            return new_offset + signature_len
    return offset

def parse_eat_profile_claim(payload, claims_offset, value_info):
    """Parse EAT Profile claim (265) with CBOR tag"""
    print(f"    Value: CBOR tag ({value_info})")
    if value_info == 111:  # OID tag
        print(f"      Tag type: OID (111)")
    
    # Parse the tagged value
    tag_value_header, new_offset = parse_cbor_header(payload, claims_offset)
    if tag_value_header:
        if tag_value_header[0] == 3:  # Text string
            profile_len = tag_value_header[1]
            profile_value = payload[new_offset:new_offset+profile_len].decode('utf-8')
            print(f"      EAT Profile OID: '{profile_value}'")
            return new_offset + profile_len
        elif tag_value_header[0] == 2:  # Byte string
            profile_len = tag_value_header[1]
            profile_bytes = payload[new_offset:new_offset+profile_len]
            profile_value = profile_bytes.decode('utf-8')
            print(f"      EAT Profile OID: '{profile_value}'")
            return new_offset + profile_len
        else:
            print(f"      Tagged value: {get_cbor_type_name(tag_value_header[0])}")
            return skip_cbor_value_simple(payload, new_offset, tag_value_header[0], tag_value_header[1])
    else:
        print(f"      Could not parse tagged value")
        return claims_offset

def parse_measurements_claim(payload, claims_offset, value_info):
    """Parse measurements claim (273) array"""
    print(f"    Value: measurements array ({value_info} elements)")
    current_offset = claims_offset
    
    # Parse each measurement format in the array
    for meas_idx in range(value_info):
        print(f"      Measurement {meas_idx + 1}:")
        result, current_offset = parse_measurement_format(payload, current_offset)
        print(f"        {result}")
    
    return current_offset

def parse_generic_claim_value(payload, claims_offset, value_type, value_info):
    """Parse generic claim values based on CBOR type"""
    if value_type == 2:  # Byte string
        if value_info <= 32:  # Show short byte strings in hex
            byte_data = payload[claims_offset:claims_offset+value_info]
            print(f"    Value: byte string ({value_info} bytes): {byte_data.hex()}")
        else:
            print(f"    Value: byte string ({value_info} bytes)")
        return claims_offset + value_info
    elif value_type == 3:  # Text string
        text_value = payload[claims_offset:claims_offset+value_info].decode('utf-8')
        print(f"    Value: text string = '{text_value}'")
        return claims_offset + value_info
    elif value_type == 0:  # Positive integer
        print(f"    Value: positive integer = {value_info}")
        return claims_offset
    elif value_type == 1:  # Negative integer
        print(f"    Value: negative integer = {-value_info - 1}")
        return claims_offset
    elif value_type == 4:  # Array (generic)
        print(f"    Value: array ({value_info} elements)")
        return skip_cbor_array(payload, claims_offset, value_info)
    elif value_type == 5:  # Map
        print(f"    Value: map ({value_info} pairs)")
        return skip_cbor_map(payload, claims_offset, value_info)
    elif value_type == 6:  # Tag (generic)
        print(f"    Value: CBOR tag ({value_info})")
        # Skip other tagged values
        tag_value_header, new_offset = parse_cbor_header(payload, claims_offset)
        if tag_value_header:
            return skip_cbor_value_simple(payload, new_offset, tag_value_header[0], tag_value_header[1])
        return claims_offset
    else:
        print(f"    Value: unknown type {value_type}")
        return claims_offset

def parse_eat_claims(payload):
    """Parse EAT claims from payload"""
    print("\n=== EAT Claims Analysis ===")
    claims_offset = 0
    claims_header, claims_offset = parse_cbor_header(payload, claims_offset)
    
    if not (claims_header and claims_header[0] == 5):  # Map
        print("EAT payload is not a CBOR map")
        return
    
    num_claims = claims_header[1]
    print(f"EAT claims: map with {num_claims} entries")
    
    # Parse each claim
    for i in range(num_claims):
        # Parse key
        key_header, claims_offset = parse_cbor_header(payload, claims_offset)
        if not key_header:
            continue
            
        key = None
        if key_header[0] == 0:  # Positive integer key
            key = key_header[1]
            claim_name = get_eat_claim_name(key)
            print(f"  Claim {i+1}: {claim_name} (key={key})")
        elif key_header[0] == 3:  # Text string key
            key_len = key_header[1]
            key = payload[claims_offset:claims_offset+key_len].decode('utf-8')
            claims_offset += key_len
            print(f"  Claim {i+1}: '{key}' (string key)")
        else:
            print(f"  Claim {i+1}: Key = unknown type {key_header[0]}")
        
        # Parse value with special handling for specific claims
        value_header, new_offset = parse_cbor_header(payload, claims_offset)
        if not value_header:
            continue
            
        value_type = value_header[0]
        value_info = value_header[1]
        
        # Special handling for specific claims
        if key == 273 and value_type == 4:  # Measurements array
            claims_offset = parse_measurements_claim(payload, new_offset, value_info)
        elif key == 265 and value_type == 6:  # EAT Profile tag
            claims_offset = parse_eat_profile_claim(payload, new_offset, value_info)
        else:
            # Generic claim value parsing
            claims_offset = parse_generic_claim_value(payload, new_offset, value_type, value_info)

def parse_cose_payload(data, offset):
    """Parse COSE Sign1 payload and return new offset"""
    header, new_offset = parse_cbor_header(data, offset)
    if not (header and header[0] == 2):  # Byte string
        return offset
        
    payload_len = header[1]
    payload = data[new_offset:new_offset+payload_len]
    print(f"Payload ({payload_len} bytes)")
    print(f"Payload first 64 bytes: {payload[:64].hex()}")
    
    # Try to parse payload as CBOR
    try:
        parse_eat_claims(payload)
        
        # Also extract claims to dictionary and save as JSON
        print("\n=== Extracting Claims to JSON ===")
        try:
            claims_dict = structured_claims_parser(payload)
        except Exception as e:
            print(f"Structured claim parsing failed: {e}")
            claims_dict = {}
        if claims_dict:
            import time, json
            timestamp = int(time.time())
            json_filename = f"eat_claims_{timestamp}.json"
            with open(json_filename, 'w') as jf:
                json.dump(claims_dict, jf, indent=2)
            print(f"Claims dictionary contains {len(claims_dict)} entries -> {json_filename}")
        else:
            print("No structured claims extracted")
    except Exception as e:
        print(f"Could not parse EAT claims: {e}")
        import traceback
        traceback.print_exc()
    
    return new_offset + payload_len

def parse_cose_payload_with_data(data, offset):
    """Parse COSE Sign1 payload and return data"""
    header, new_offset = parse_cbor_header(data, offset)
    if not (header and header[0] == 2):  # Byte string
        return offset, None
        
    payload_len = header[1]
    payload = data[new_offset:new_offset+payload_len]
    print(f"Payload ({payload_len} bytes)")
    print(f"Payload first 64 bytes: {payload[:64].hex()}")
    
    # Try to parse payload as CBOR
    try:
        parse_eat_claims(payload)
    except Exception as e:
        print(f"Could not parse EAT claims: {e}")
        import traceback
        traceback.print_exc()
    
    return new_offset + payload_len, payload

def parse_cose_signature_with_data(data, offset):
    """Parse COSE Sign1 signature and return data"""
    if offset < len(data):
        header, new_offset = parse_cbor_header(data, offset)
        if header and header[0] == 2:  # Byte string
            signature_len = header[1]
            signature = data[new_offset:new_offset+signature_len]
            print(f"Signature ({signature_len} bytes): {signature.hex()}")
            # Enhanced signature analysis
            analyze_cose_signature(signature)
            return signature
    return None

def parse_cose_sign1_structure(cose_data):
    """Parse the main COSE Sign1 structure"""
    print("\n=== COSE Sign1 Structure Analysis ===")
    offset = 0
    
    # Parse the main array structure
    header, offset = parse_cbor_header(cose_data, offset)
    if not header:
        print("Could not parse COSE structure header")
        return
        
    major_type, array_length = header
    if major_type != 4:  # Array
        print(f"Unexpected CBOR structure: major type {major_type}")
        return
        
    print(f"COSE Sign1 array with {array_length} elements")
    
    # Store data for signature validation
    validation_data = {}
    
    # Element 1: Protected headers (bstr)
    offset, protected_headers = parse_cose_protected_headers_with_data(cose_data, offset)
    validation_data['protected_headers'] = protected_headers
    
    # Element 2: Unprotected headers (map)
    offset, certificate = parse_cose_unprotected_headers_with_data(cose_data, offset)
    validation_data['certificate'] = certificate
    
    # Element 3: Payload (bstr)
    offset, payload = parse_cose_payload_with_data(cose_data, offset)
    validation_data['payload'] = payload
    
    # Element 4: Signature (bstr)
    signature = parse_cose_signature_with_data(cose_data, offset)
    validation_data['signature'] = signature
    
    # Perform signature validation if we have all the data
    if all(key in validation_data and validation_data[key] is not None 
           for key in ['protected_headers', 'payload', 'signature', 'certificate']):
        validate_cose_signature(
            validation_data['protected_headers'],
            validation_data['payload'], 
            validation_data['signature'],
            validation_data['certificate']
        )

def decode_eat_token(file_path):
    """Decode an EAT token that may be wrapped in CBOR tags"""
    if not os.path.exists(file_path):
        print(f"Error: File '{file_path}' not found in the current directory.")
        print(f"Current directory: {os.getcwd()}")
        return
    
    try:
        with open(file_path, "rb") as f:
            data = f.read()
        
        print(f"File size: {len(data)} bytes")
        print(f"First 16 bytes (hex): {data[:16].hex()}")
        
        # Skip any CBOR tags to get to the COSE message
        cose_data = skip_cbor_tags(data)
        
        if len(cose_data) != len(data):
            print(f"Skipped {len(data) - len(cose_data)} bytes of CBOR tags")
            print(f"COSE data first 16 bytes: {cose_data[:16].hex()}")
        
        # Parse the COSE Sign1 structure
        parse_cose_sign1_structure(cose_data)
        
    except Exception as e:
        print(f"Error decoding file: {e}")
        import traceback
        traceback.print_exc()

def verify_claims(json_claims_path: str, expected_nonce: bytes | None, reference_triples_path: str | None) -> dict:
    """Generic claims verification.

    Parameters:
      json_claims_path: Path to claims JSON produced by fast extraction.
      expected_nonce: Raw expected nonce bytes (or None to skip nonce check).
      reference_triples_path: Path to reference triples JSON (or None).

    Behavior:
      * Loads claims JSON once.
      * If expected_nonce provided, locate 'nonce' claim and compare (hex or utf-8 heuristic).
      * If reference_triples_path provided, load reference and compare digests/integrity using
        decode_eat_claims_json.compare_claims_to_reference (which expects full claims dict format).
    Returns True only if all requested verifications succeed.
    """
    ok = True
    nonce_ok: bool | None = None
    ref_ok: bool | None = None
    try:
        with open(json_claims_path, 'r') as jf:
            claims = json.load(jf)
    except Exception as e:  # noqa: BLE001
        print(f"[CLAIMS] ERROR loading claims JSON '{json_claims_path}': {e}")
        return False
    print("\n=== 5) Verify Claims ===")
    # --- Nonce verification ---
    if expected_nonce is not None:
        nonce_val = claims.get('nonce')
        def _to_bytes(v):
            if v is None:
                return None
            if isinstance(v, bytes):
                return v
            if isinstance(v, str):
                s = v.strip()
                if len(s) % 2 == 0 and s != '' and all(c in '0123456789abcdefABCDEF' for c in s):
                    try:
                        return bytes.fromhex(s)
                    except Exception:  # noqa: BLE001
                        pass
                return s.encode('utf-8', 'ignore')
            if isinstance(v, int):
                if v == 0:
                    return b'\x00'
                out = []
                x = v
                while x:
                    out.append(x & 0xFF)
                    x >>= 8
                return bytes(reversed(out))
            return bytes(str(v), 'utf-8')
        actual = _to_bytes(nonce_val)
        if actual is None:
            print('[CLAIMS] Nonce claim missing')
            ok = False
            nonce_ok = False
        elif actual == expected_nonce:
            print(f"[CLAIMS] Nonce OK ({actual.hex()})")
            nonce_ok = True
        else:
            print(f"[CLAIMS] Nonce MISMATCH expected={expected_nonce.hex()} actual={actual.hex()} (len exp={len(expected_nonce)} act={len(actual)})")
            ok = False
            nonce_ok = False

    # --- Reference triples comparison ---
    if reference_triples_path:
        try:
            from decode_eat_claims_json import compare_claims_to_reference  # type: ignore
        except Exception as e:  # noqa: BLE001
            print(f"[REF] Unable to import comparison helper: {e}")
            return {"overall_ok": False, "nonce_ok": nonce_ok, "reference_match": False}
        try:
            with open(reference_triples_path, 'r') as rf:
                reference = json.load(rf)
        except Exception as e:  # noqa: BLE001
            print(f"[REF] Failed to load reference file '{reference_triples_path}': {e}")
            return {"overall_ok": False, "nonce_ok": nonce_ok, "reference_match": False}
        cmp_result = compare_claims_to_reference(claims, reference)
        mismatches = cmp_result.get('mismatches') or []
        if cmp_result.get('matches'):
            print('[REF] Reference digest comparison: ALL MATCH')
            ref_ok = True
            # If user did not ask for full detail, emit a concise summary of matched components
            ref_detail = globals().get('REFERENCE_DETAIL') or globals().get('VERBOSE_REFERENCE_DETAIL')
            if not ref_detail:
                try:
                    ref_triples = reference.get('reference_triples', []) if isinstance(reference, dict) else []
                    printed_header = False
                    for t in ref_triples:
                        cid = t.get('class_id')
                        meas = t.get('measurements') or []
                        if not printed_header:
                            print('[REF] Matched components:')
                            printed_header = True
                        # Print per measurement first digest
                        if not meas:
                            print(f"    class_id={cid} (no measurements)")
                            continue
                        for m in meas:
                            if not isinstance(m, dict):
                                continue
                            mkey = m.get('mkey')
                            digests = m.get('digests') or []
                            first_digest = digests[0] if digests else None
                            # shorten digest for readability
                            if isinstance(first_digest, str) and len(first_digest) > 24:
                                short = first_digest[:24] + '...'
                            else:
                                short = first_digest
                            print(f"    class_id={cid} mkey={mkey} digest={short}")
                except Exception:
                    pass
        else:
            print('[REF] Reference digest comparison: MISMATCHES found')
            try:
                print(json.dumps(mismatches, indent=2))
            except Exception:  # noqa: BLE001
                print(mismatches)
            ok = False
            ref_ok = False
        # Optional detailed per-measurement status
        ref_detail = globals().get('REFERENCE_DETAIL') or globals().get('VERBOSE_REFERENCE_DETAIL')
        if ref_detail:
            # Build quick lookup for mismatches by (class_id, mkey)
            mismatch_index = {}
            for m in mismatches:
                cid = m.get('class_id')
                mk = m.get('mkey')
                key = (cid, mk)
                mismatch_index.setdefault(key, []).append(m.get('issue'))
            ref_triples = reference.get('reference_triples', []) if isinstance(reference, dict) else []
            print('[REF] Detailed comparison:')
            for t in ref_triples:
                cid = t.get('class_id')
                meas_list = t.get('measurements') or []
                if not meas_list:
                    line = f"  class_id={cid}: (no measurements in reference)"
                    print(line)
                    continue
                print(f"  class_id={cid}:")
                for m in meas_list:
                    mk = m.get('mkey')
                    digests = m.get('digests') or []
                    first_digest = digests[0] if digests else None
                    issues = mismatch_index.get((cid, mk))
                    if issues:
                        print(f"    mkey={mk} DIGEST={first_digest} -> FAIL ({'/'.join(issues)})")
                    else:
                        # Only mark success if comparison covered this measurement; if unmatched claim side could hide issues.
                        status = 'OK' if ref_ok else 'OK*'  # OK* when overall failed but this one not listed
                        print(f"    mkey={mk} DIGEST={first_digest} -> {status}")
    return {"overall_ok": ok, "nonce_ok": nonce_ok, "reference_match": ref_ok}

def main():
    """Main function to decode EAT tokens from command line arguments with optional nonce verification."""

    if len(sys.argv) < 2:
        print("Usage: python3 decode.py <eat_token_file> [options]\n")
        print("Core:")
        print("  --json                   Fast path: extract claims JSON (supports verification)")
        print("  --verbose                Verbose structural decode (can combine with --json)")
        print("  --nonce <hex|utf8>       Expected nonce (hex if even-length & hex chars, else UTF-8)")
        print()
        print("Reference / Comparison:")
        print("  --emit-reference [FILE]  Emit reference triples (default <token>_reference.json if FILE omitted)")
        print("  --reference <FILE>       Compare evidence digests to reference triples")
        print("  --reference-mkeys <csv>  Only include specified mkeys when emitting reference (e.g. 0,1,0x2)")
        print("  --print-reference        Also print generated reference JSON")
        print("  --reference-detail       Print per-measurement comparison status (even when all match)")
        print()
        print("Verification Output Control:")
        print("  --no-embed-verification  Do not embed verification summary into claims JSON")
        print()
        print("Examples:")
        print("  python3 decode.py token.cbor --json --nonce deadbeef")
        print("  python3 decode.py token.cbor --json --emit-reference")
        print("  python3 decode.py token.cbor --json --emit-reference ref.json --print-reference")
        print("  python3 decode.py token.cbor --json --reference ref.json --nonce 746573745f6e6f6e6365")
        sys.exit(1)

    # Basic arg parsing (preserve existing style without introducing argparse)
    args = sys.argv[1:]
    file_path = args[0]
    json_flag = "--json" in args
    verbose = "--verbose" in args
    reference_detail = "--reference-detail" in args
    # Expose to verify_claims via global
    globals()['REFERENCE_DETAIL'] = reference_detail
    if reference_detail and verbose:
        globals()['VERBOSE_REFERENCE_DETAIL'] = True
    expected_nonce_arg = None
    if "--nonce" in args:
        try:
            idx = args.index("--nonce")
            expected_nonce_arg = args[idx + 1]
        except Exception:
            print("ERROR: --nonce supplied without a value")
            sys.exit(2)

    def _normalize_nonce_input(val: str) -> bytes:
        # If looks like hex (even length, hex chars only) treat as hex
        hs = val.lower().strip()
        if len(hs) % 2 == 0 and all(c in '0123456789abcdef' for c in hs):
            try:
                return bytes.fromhex(hs)
            except Exception:
                pass
        # else interpret as utf-8 text
        return val.encode('utf-8')

    expected_nonce_bytes = _normalize_nonce_input(expected_nonce_arg) if expected_nonce_arg else None
    emit_ref_path = None
    if "--emit-reference" in args:
        try:
            idx_er = args.index("--emit-reference")
            # If next arg absent or starts with '--', use default basename
            if idx_er == len(args) - 1 or args[idx_er + 1].startswith('--'):
                base = os.path.splitext(os.path.basename(file_path))[0]
                emit_ref_path = f"{base}_reference.json"
            else:
                emit_ref_path = args[idx_er + 1]
        except Exception:
            base = os.path.splitext(os.path.basename(file_path))[0]
            emit_ref_path = f"{base}_reference.json"
    reference_path = None
    if "--reference" in args:
        try:
            reference_path = args[args.index("--reference") + 1]
        except Exception:
            print("ERROR: --reference supplied without a file path")
            sys.exit(2)
    print_reference = "--print-reference" in args
    mkey_filter_arg = None
    if "--reference-mkeys" in args:
        try:
            mkey_filter_arg = args[args.index("--reference-mkeys") + 1]
        except Exception:
            print("ERROR: --reference-mkeys supplied without value (comma separated)")
            sys.exit(2)
    include_mkeys: set[int] | None = None
    if mkey_filter_arg:
        include_mkeys = set()
        for part in mkey_filter_arg.split(','):
            p = part.strip()
            if not p:
                continue
            try:
                if p.startswith('0x'):
                    include_mkeys.add(int(p,16))
                else:
                    include_mkeys.add(int(p))
            except Exception:
                print(f"[REF] Ignoring invalid mkey '{p}'")
    no_embed_verification = "--no-embed-verification" in args

    # Color support
    def _color(enabled: bool, text: str, fg: str):
        if not enabled:
            return text
        colors = {
            'red': '\u001b[31m', 'green': '\u001b[32m', 'yellow': '\u001b[33m',
            'cyan': '\u001b[36m', 'magenta': '\u001b[35m'
        }
        reset = '\u001b[0m'
        return f"{colors.get(fg,'')}{text}{reset}" if fg in colors else text
    use_color = sys.stdout.isatty() and 'NO_COLOR' not in os.environ
    
    # Configure logging based on verbose mode
    if verbose:
        logging.basicConfig(
            level=logging.DEBUG,
            format='%(levelname)s: %(message)s',
            force=True  # Override any existing configuration
        )
    else:
        logging.basicConfig(
            level=logging.INFO,
            format='%(message)s',
            force=True
        )
    
    # Behavior matrix:
    #  --json (no --verbose): run fast JSON extraction (info-level minimal output)
    #  --json --verbose: perform full decode (all prints + debug logging) AND emit JSON file
    #  (no --json): original full decode
    def _post_parse_reference_ops(claims_file_path: str):
        # If reference operations requested, re-parse structured claims and act
        if not (emit_ref_path or reference_path):
            return True
        try:
            from decode_eat_claims_json import parse_eat_claims_to_dict, build_reference_from_claims, compare_claims_to_reference
        except Exception as e:
            print(f"[REF] Unable to import reference helpers: {e}")
            return False
        try:
            with open(claims_file_path,'rb') as f:
                raw = f.read()
            cose_data = skip_cbor_tags(raw)
            # extract payload (reuse earlier approach)
            off = 0
            hdr, off = parse_cbor_header(cose_data, off)
            if not hdr or hdr[0] != 4:
                print('[REF] Not a COSE Sign1 array; cannot build reference')
                return False
            # skip protected
            _, off = parse_cbor_header(cose_data, off)
            # skip unprotected map generically
            uh, off2 = parse_cbor_header(cose_data, off)
            if uh and uh[0] == 5:
                off = skip_cbor_value_simple(cose_data, off, uh[0], uh[1])
            else:
                off = off2
            ph, offp = parse_cbor_header(cose_data, off)
            if not ph or ph[0] != 2:
                print('[REF] Payload not found for reference ops')
                return False
            plen = ph[1]
            payload = cose_data[offp:offp+plen]
            claims_dict = parse_eat_claims_to_dict(payload)
            # Build & write reference if requested
            if emit_ref_path:
                ref_obj = build_reference_from_claims(claims_dict)
                import json
                with open(emit_ref_path,'w') as rf:
                    json.dump(ref_obj, rf, indent=2)
                print(f"[REF] Wrote reference triples to {emit_ref_path} ({len(ref_obj.get('reference_triples', []))} entries)")
            if reference_path:
                import json
                try:
                    with open(reference_path,'r') as rf:
                        ref_loaded = json.load(rf)
                except Exception as e:
                    print(f"[REF] Failed to load reference file: {e}")
                    return False
                cmp_result = compare_claims_to_reference(claims_dict, ref_loaded)
                if cmp_result.get('matches'):
                    print('[REF] Reference digest comparison: ALL MATCH')
                else:
                    print('[REF] Reference digest comparison: MISMATCHES found')
                    import json
                    print(json.dumps(cmp_result.get('mismatches'), indent=2))
                return cmp_result.get('matches')
            return True
        except Exception as e:
            print(f"[REF] Error during reference operations: {e}")
            return False

    if json_flag and not verbose:
        print(f"=== Parsing EAT CWT (json fast path) from {file_path} ===")
        extract_claims_to_json_only(file_path, skip_cbor_tags, parse_cbor_header, verbose)
        # Determine generated JSON claims file path (naming mirrors extract_claims_to_json_only)
        json_claims_path = f"{os.path.splitext(os.path.basename(file_path))[0]}_claims.json"

        # If emit-reference requested, build from already saved claims JSON
        ref_ok = True
        if emit_ref_path:
            try:
                from decode_eat_claims_json import build_reference_from_claims  # type: ignore
                with open(json_claims_path, 'r') as jf:
                    _claims = json.load(jf)
                ref_obj = build_reference_from_claims(_claims, include_mkeys)
                with open(emit_ref_path, 'w') as rf:
                    json.dump(ref_obj, rf, indent=2)
                print(f"[REF] Wrote reference triples to {emit_ref_path} ({len(ref_obj.get('reference_triples', []))} entries)")
                if print_reference:
                    print("[REF] Reference JSON:")
                    print(json.dumps(ref_obj, indent=2))
            except Exception as e:  # noqa: BLE001
                print(f"[REF] Failed to emit reference: {e}")
                ref_ok = False

        verify_result = verify_claims(json_claims_path, expected_nonce_bytes, reference_path)
        if not no_embed_verification:
            try:
                with open(json_claims_path, 'r') as jf:
                    claims_loaded = json.load(jf)
                claims_loaded['verification'] = {
                    'nonce_ok': verify_result.get('nonce_ok'),
                    'reference_match': verify_result.get('reference_match'),
                    'overall_ok': verify_result.get('overall_ok')
                }
                with open(json_claims_path, 'w') as jf:
                    json.dump(claims_loaded, jf, indent=2)
            except Exception as e:  # noqa: BLE001
                print(f"[WARN] Unable to embed verification summary: {e}")
        status_line = (
            f"[STATUS] overall={'OK' if verify_result.get('overall_ok') else 'FAIL'} "
            f"nonce={verify_result.get('nonce_ok')} ref={verify_result.get('reference_match')}"
        )
        print(_color(use_color, status_line, 'green' if verify_result.get('overall_ok') else 'red'))

        if not (verify_result.get('overall_ok') and ref_ok):
            sys.exit(3)
    elif json_flag and verbose:
        # Full decode plus JSON extraction
        print(f"=== Decoding (full) + JSON export for {file_path} ===")
        decode_eat_token(file_path)
        # After full decode, re-run structured extraction on payload file directly for deterministic JSON
        try:
            with open(file_path, 'rb') as f:
                raw = f.read()
            cose_data = skip_cbor_tags(raw)
            # Minimal reuse: find payload quickly (we already have logic, but reuse decode path would re-print).
            # Simpler: rely on structured parser if token is EAT Sign1 with known layout; catch errors quietly.
            from decode_eat_claims_json import parse_eat_claims_to_dict as _claims_parser
            claims_dict = {}
            try:
                # naive: search for payload bstr after first array header; this is already done in decode_eat_claims_json when given payload only
                # We cannot easily extract payload without re-parsing; fallback: if structured parser raises just skip.
                # So re-run the extraction helper used by old path (just call extract_claims_to_json_only logic inline)
                # For simplicity call extract_claims_to_json_only again (won't hurt) but mark verbose False to suppress duplication.
                extract_claims_to_json_only(file_path, skip_cbor_tags, parse_cbor_header, False)
            except Exception:
                pass
            # After full decode create JSON (fast extract) and then perform unified verification if requested
            json_claims_path = f"{os.path.splitext(os.path.basename(file_path))[0]}_claims.json"
            if emit_ref_path:
                try:
                    from decode_eat_claims_json import build_reference_from_claims  # type: ignore
                    with open(json_claims_path, 'r') as jf:
                        _claims = json.load(jf)
                    ref_obj = build_reference_from_claims(_claims, include_mkeys)
                    with open(emit_ref_path, 'w') as rf:
                        json.dump(ref_obj, rf, indent=2)
                    print(f"[REF] Wrote reference triples to {emit_ref_path} ({len(ref_obj.get('reference_triples', []))} entries)")
                    if print_reference:
                        print("[REF] Reference JSON:")
                        print(json.dumps(ref_obj, indent=2))
                except Exception as e:
                    print(f"[REF] Failed to emit reference: {e}")
            verify_result = verify_claims(json_claims_path, expected_nonce_bytes, reference_path)
            if verify_result and not no_embed_verification:
                try:
                    with open(json_claims_path, 'r') as jf:
                        claims_loaded = json.load(jf)
                    claims_loaded['verification'] = {
                        'nonce_ok': verify_result.get('nonce_ok'),
                        'reference_match': verify_result.get('reference_match'),
                        'overall_ok': verify_result.get('overall_ok')
                    }
                    with open(json_claims_path, 'w') as jf:
                        json.dump(claims_loaded, jf, indent=2)
                except Exception:
                    pass
            status_line = (
                f"[STATUS] overall={'OK' if verify_result.get('overall_ok') else 'FAIL'} "
                f"nonce={verify_result.get('nonce_ok')} ref={verify_result.get('reference_match')}"
            )
            print(_color(use_color, status_line, 'green' if verify_result.get('overall_ok') else 'red'))
            if not verify_result.get('overall_ok'):
                sys.exit(3)
        except Exception as e:
            print(f"[WARN] JSON export after full decode failed: {e}")
    else:
        print(f"=== Decoding {file_path} ===")
        decode_eat_token(file_path)
        # Optional reference ops & verification (create JSON if needed)
        if emit_ref_path or reference_path or expected_nonce_bytes is not None:
            # Produce claims JSON via fast extractor
            extract_claims_to_json_only(file_path, skip_cbor_tags, parse_cbor_header, False)
            json_claims_path = f"{os.path.splitext(os.path.basename(file_path))[0]}_claims.json"
            if emit_ref_path:
                try:
                    from decode_eat_claims_json import build_reference_from_claims  # type: ignore
                    with open(json_claims_path, 'r') as jf:
                        _claims = json.load(jf)
                    ref_obj = build_reference_from_claims(_claims, include_mkeys)
                    with open(emit_ref_path, 'w') as rf:
                        json.dump(ref_obj, rf, indent=2)
                    print(f"[REF] Wrote reference triples to {emit_ref_path} ({len(ref_obj.get('reference_triples', []))} entries)")
                    if print_reference:
                        print("[REF] Reference JSON:")
                        print(json.dumps(ref_obj, indent=2))
                except Exception as e:
                    print(f"[REF] Failed to emit reference: {e}")
            verify_result = verify_claims(json_claims_path, expected_nonce_bytes, reference_path)
            if verify_result and not no_embed_verification:
                try:
                    with open(json_claims_path, 'r') as jf:
                        claims_loaded = json.load(jf)
                    claims_loaded['verification'] = {
                        'nonce_ok': verify_result.get('nonce_ok'),
                        'reference_match': verify_result.get('reference_match'),
                        'overall_ok': verify_result.get('overall_ok')
                    }
                    with open(json_claims_path, 'w') as jf:
                        json.dump(claims_loaded, jf, indent=2)
                except Exception:
                    pass
            status_line = (
                f"[STATUS] overall={'OK' if verify_result.get('overall_ok') else 'FAIL'} "
                f"nonce={verify_result.get('nonce_ok')} ref={verify_result.get('reference_match')}"
            )
            print(_color(use_color, status_line, 'green' if verify_result.get('overall_ok') else 'red'))
            if not verify_result.get('overall_ok'):
                sys.exit(3)

if __name__ == "__main__":
    main()